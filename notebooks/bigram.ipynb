{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM From scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe000c97c70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Text Small Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n",
      "the text has a length of 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "# Open The text file\n",
    "with open(\"../input.txt\",\"r\", encoding = \"utf-8\") as f:\n",
    "    text = f.read()\n",
    "print(text[:100])\n",
    "print(f'the text has a length of {len(text)} characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Tokenizer and Encoder/Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize : Transform Text --> Integers\n",
    "\n",
    "Short size of vocabulary but Long sequence length : Character-level Tokenizer\n",
    "Simple Tokeinizer : with a mapping of characters to integers using the position in chars (The one we will use)\n",
    "\n",
    "Long size of vocabulary but Short sequence length : sub-word/Word-level Tokenizer\n",
    "Google Tokenizer : SentencePiece : sub-words units level tokenizer\n",
    "OPENAI : Tiktoken : BPE Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# Characters of the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder : Transform text --> Vectors/integers $\\newline$\n",
    "Decoder : Transform Vectors/integers --> text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48, 43, 1, 57, 59, 47, 57, 1, 27, 57, 47, 56, 47, 57]\n",
      "je suis Osiris\n"
     ]
    }
   ],
   "source": [
    "# Create the mapping from characters to integer\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "def encode(s): return [stoi[c] for c in s]\n",
    "def decode(L): return \"\".join([itos[i] for i in L])\n",
    "\n",
    "print(encode(\"je suis Osiris\"))\n",
    "print(decode([48, 43, 1, 57, 59, 47, 57, 1, 27, 57, 47, 56, 47, 57]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using **PyTorch Tensors** to store text as **embeddings**\n",
    "\n",
    "- **Ease of use**: All modules (**nn.Embedding**, **nn.Linear**, **Transformer**, etc.) expect **torch.Tensor**.  \n",
    "\n",
    "- **GPU/TPU**: Once it’s a **tensor**, it can be moved to **GPU/TPU** with `x.to(\"cuda\")` or `x.to(\"tpu\")`. **NumPy** only works on **CPU**.  \n",
    "\n",
    "- **Backward / Autograd**: **PyTorch tensors** keep track of operations for **automatic gradient computation**.  \n",
    "\n",
    "  **Example**:  \n",
    "  ```python\n",
    "  import torch\n",
    "\n",
    "  x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "  y = x * 2\n",
    "  y.backward(torch.ones_like(x))\n",
    "  print(x.grad)  # x remembers its expression with respect to y allowing it to compute x.grad automatically\n",
    "\n",
    "    ```\n",
    "- `x` remembers its expression with respect to `y`, which allows computing **x.grad** automatically.  \n",
    "\n",
    "- **Integrated tensors** in **PyTorch** (with **optimizers** like **Adam**, **SGD**, **loss functions**, training on **mini-batches**).  \n",
    "\n",
    "- **PyTorch** is optimized in **C++/CUDA** → ultra-fast **matrix computations**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text))\n",
    "print(data.shape,data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting train and test\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Chunk = split the text** into smaller pieces (e.g. paragraphs of **300–500 tokens**).\n",
    "\n",
    "### 🔹 Why chunk?\n",
    "- **Context window limit**: LLMs can only read a fixed number of tokens (2k, 4k, 32k...).$\\newline$\n",
    "--> Chunking helps fit large texts into the model's context window.$\\newline$\n",
    "--> Attention mechanisms in transformers have quadratic complexity with respect to input length, so shorter inputs are more efficient.$\\newline$\n",
    "- **Memory efficiency**: smaller chunks use less memory during processing.\n",
    "- **Better local coherence**: smaller chunks keep the meaning clear.  \n",
    "- **Efficient search (RAG)**: embeddings are stored per chunk, making retrieval precise.  \n",
    "- **Performance**: faster similarity search + less dilution in attention.  \n",
    "\n",
    "Rule of thumb: chunks of **200–500 tokens** usually work best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: <bound method Kernel.raw_input of <ipykernel.ipkernel.IPythonKernel object at 0x7ff185012100>>, context: tensor([18])\n",
      "input: <bound method Kernel.raw_input of <ipykernel.ipkernel.IPythonKernel object at 0x7ff185012100>>, context: tensor([18, 47])\n",
      "input: <bound method Kernel.raw_input of <ipykernel.ipkernel.IPythonKernel object at 0x7ff185012100>>, context: tensor([18, 47, 56])\n",
      "input: <bound method Kernel.raw_input of <ipykernel.ipkernel.IPythonKernel object at 0x7ff185012100>>, context: tensor([18, 47, 56, 57])\n",
      "input: <bound method Kernel.raw_input of <ipykernel.ipkernel.IPythonKernel object at 0x7ff185012100>>, context: tensor([18, 47, 56, 57, 58])\n",
      "input: <bound method Kernel.raw_input of <ipykernel.ipkernel.IPythonKernel object at 0x7ff185012100>>, context: tensor([18, 47, 56, 57, 58,  1])\n",
      "input: <bound method Kernel.raw_input of <ipykernel.ipkernel.IPythonKernel object at 0x7ff185012100>>, context: tensor([18, 47, 56, 57, 58,  1, 15])\n",
      "input: <bound method Kernel.raw_input of <ipykernel.ipkernel.IPythonKernel object at 0x7ff185012100>>, context: tensor([18, 47, 56, 57, 58,  1, 15, 47])\n"
     ]
    }
   ],
   "source": [
    "# Exemple\n",
    "block_size = 8\n",
    "x = train_data[:block_size] # input of the transformer : first block size characters\n",
    "y = train_data[1:block_size+1] # next block size character offset by one --> target for each pos\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1] # all the tokens before the token at t\n",
    "    target = y[t]     # expected token at t\n",
    "    print(f'input: {context}, target: {y[t]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching\n",
    "\n",
    "- **Batch** = a set of sequences processed in parallel on the GPU.  \n",
    "- **Main purpose**: speed up training with GPU parallelization.  \n",
    "- Each sequence in the batch is **independent**:  \n",
    "  - The loss is calculated for each sequence, then combined to update the model weights.  \n",
    "- Batching is used **only to process multiple sequences in parallel** for GPU efficiency, **without sharing context between them**.\n",
    "- **Weight update**: after computing the losses for all the sequences in the batch, **the gradients are combined to update the model’s weights.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------Inputs-------------\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "-------------Targets-------------\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 4  # Number of sequences (sentences) processed in parallel\n",
    "block_size = 8  # Maximum length of a sequence in tokens (model context window)\n",
    "\n",
    "def get_batch(split):\n",
    "    # Generate small batch of data of inputs x and target y\n",
    "    data = train_data if split =='train' else val_data\n",
    "    # Choice of random offset (starting point) during training --> Avoid overfitting (<== if we start always at the same point)\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])       # context (stacked by row in a batch_size x block_size Tensor)\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])   # target/prediction\n",
    "    return x,y\n",
    "\n",
    "# Visualisation\n",
    "xb,yb = get_batch('train')\n",
    "print('-------------Inputs-------------')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('-------------Targets-------------')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "print('---------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = [24], target = 43\n",
      "input = [24, 43], target = 58\n",
      "input = [24, 43, 58], target = 5\n",
      "input = [24, 43, 58, 5], target = 57\n",
      "input = [24, 43, 58, 5, 57], target = 1\n",
      "input = [24, 43, 58, 5, 57, 1], target = 46\n",
      "input = [24, 43, 58, 5, 57, 1, 46], target = 43\n",
      "input = [24, 43, 58, 5, 57, 1, 46, 43], target = 39\n",
      "input = [44], target = 53\n",
      "input = [44, 53], target = 56\n",
      "input = [44, 53, 56], target = 1\n",
      "input = [44, 53, 56, 1], target = 58\n",
      "input = [44, 53, 56, 1, 58], target = 46\n",
      "input = [44, 53, 56, 1, 58, 46], target = 39\n",
      "input = [44, 53, 56, 1, 58, 46, 39], target = 58\n",
      "input = [44, 53, 56, 1, 58, 46, 39, 58], target = 1\n",
      "input = [52], target = 58\n",
      "input = [52, 58], target = 1\n",
      "input = [52, 58, 1], target = 58\n",
      "input = [52, 58, 1, 58], target = 46\n",
      "input = [52, 58, 1, 58, 46], target = 39\n",
      "input = [52, 58, 1, 58, 46, 39], target = 58\n",
      "input = [52, 58, 1, 58, 46, 39, 58], target = 1\n",
      "input = [52, 58, 1, 58, 46, 39, 58, 1], target = 46\n",
      "input = [25], target = 17\n",
      "input = [25, 17], target = 27\n",
      "input = [25, 17, 27], target = 10\n",
      "input = [25, 17, 27, 10], target = 0\n",
      "input = [25, 17, 27, 10, 0], target = 21\n",
      "input = [25, 17, 27, 10, 0, 21], target = 1\n",
      "input = [25, 17, 27, 10, 0, 21, 1], target = 54\n",
      "input = [25, 17, 27, 10, 0, 21, 1, 54], target = 39\n"
     ]
    }
   ],
   "source": [
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"input = {context.tolist()}, target = {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram Model : simplest Language Model\n",
    "\n",
    "- A **bigram model** is a very simple language model that predicts a token based **only on the previous token**.  \n",
    "- **Unigram model** → assumes each token is independent.  \n",
    "- **Bigram model** → probability of a token depends only on the one before it.  \n",
    "- **N-gram model** → generalization where the token depends on the previous *n-1* tokens.  \n",
    "\n",
    "\n",
    "### Formula\n",
    "\n",
    "For a sequence of tokens $ w_1, w_2, \\dots, w_n $:  \n",
    "\n",
    "$$\n",
    "P(w_1, w_2, \\dots, w_n) \\approx \\prod_{i=2}^n P(w_i \\mid w_{i-1})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Reminder: Role of `nn.Embedding` in PyTorch\n",
    "\n",
    "- **`nn.Embedding(num_embeddings, embedding_dim)`** = a **lookup table** (similar to dictionnary) that maps token indices to vectors.  \n",
    "\n",
    "- Each token (represented by an **integer index**) is associated with a **learnable vector**.  \n",
    "\n",
    "- In a **language model**:\n",
    "  - Input: token indices (`idx`).\n",
    "  - Output: embeddings (vectors) that represent each token.  \n",
    "\n",
    "- Example with `nn.Embedding(vocab_size, vocab_size)` in a Bigram model:\n",
    "  - Each token index is mapped to a **vector of size `vocab_size`**.\n",
    "  - This vector acts as the **logits** (scores) for predicting the **next token**.  \n",
    "  - Applying **softmax** on these logits gives a probability distribution over the vocabulary.  \n",
    "\n",
    "➡️ In short: **`nn.Embedding` converts discrete tokens into continuous vectors** that the model can use for learning and prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loss : Evaluating the quality of the model's predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLangaugeModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        # Each token is associated to a vocab_size dimension vector \n",
    "        self.token_embedding_table = nn.Embedding(vocab_size,vocab_size)\n",
    "\n",
    "    def forward(self,idx, targets = None):\n",
    "        \n",
    "        # dim of idx and target = (B,T)\n",
    "        logits = self.token_embedding_table(idx) # size (B,T,C)\n",
    "        # Computation of the loss\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            #A. reshapping tensors to adapt them to Pytorch.Cross Entropy input size requirement\n",
    "            B,T,C = logits.shape\n",
    "            logits = logits.view(B*T,C)\n",
    "            targets = targets.view(B*T)\n",
    "            #B.Computation of the Loss\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return loss, logits\n",
    "    \n",
    "    def generate(self,idx,max_new_tokens):\n",
    "        # idx = (B,T) array of indices in the current context\n",
    "        # Goal : \n",
    "        for _ in range(max_new_tokens):\n",
    "            # get predictions\n",
    "            loss, logits = self(idx) # loss = null because no targets\n",
    "            # Bigram --> Focusing only on the last token\n",
    "            logits = logits[:,-1,:] # size (B,C) (a plane in the cubic matrix)\n",
    "            # Getting probabilities with softmax\n",
    "            probs = F.softmax(logits,dim = -1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs,num_samples=1) # size = (B,1) = the prediction for what come next for each line\n",
    "            # Append it to the running sequence\n",
    "            idx = torch.cat((idx,idx_next), dim=1) #(B,T+1) we add the last token\n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.4399, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "m = BigramLangaugeModel(vocab_size)\n",
    "# Calling the model directly (m(xb, yb)) automatically invokes forward with PyTorch hooks\n",
    "# This ensures proper handling of autograd, hooks, and training/evaluation modes\n",
    "loss, logits  = m(xb,yb)\n",
    "print(logits.shape) # size B = batch_size, T = block_size, C = vocab_size\n",
    "print(loss) # around -ln(1/65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "?ACVWq-AJAhMrXfLgNVTNySlxdRkdbm\n",
      "XL\n",
      "qM?RbO!PH PUWHVW3aYaepXWYxhnGCbgNzm;WSYtw?mzKSi:-:y!&'3voqcm$cISC\n"
     ]
    }
   ],
   "source": [
    "# Testing the first llm\n",
    "print(decode(m.generate(idx = torch.zeros(1,1, dtype = torch.long), max_new_tokens = 100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1st Conclusion:\n",
    "- the prediction doesn't take the context into account : it only predicts the next token based on the previous one, ignoring the context of the entire sequence.\n",
    "\n",
    "- The generated text is not deterministic: the next token is sampled randomly according to the probability distribution, so it can be incoherent and change with each generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Bigram Model \n",
    "To make it less random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Pytorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(),lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.514085531234741\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for steps in range(20000):\n",
    "    # Sample a batch of data\n",
    "    xb,yb = get_batch('train')\n",
    "\n",
    "    # Evaluate the loss\n",
    "    loss, logits = m(xb,yb)\n",
    "    optimizer.zero_grad(set_to_none = True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Whablo w.\n",
      "DYou thouivif s he y anere tla her.\n",
      "Y keslos, celd d uick bepat fourerubefo; lds Prd.\n",
      "Whiv\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros(1,1, dtype = torch.long), max_new_tokens = 100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion : \n",
    "- Better than random (better structure, less hazardous character succession) but still not great\n",
    "- The bigram model captures some local structure but lacks long-range context.\n",
    "\n",
    "This is the simplest possible model, but it is very limited in its ability to generate coherent text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
