{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ff18856d770>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Text Small Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open The text file\n",
    "with open(\"input.txt\",\"r\", encoding = \"utf-8\") as f:\n",
    "    text = f.read()\n",
    "print(text[:100])\n",
    "print(f'the text has a length of {len(text)} characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Tokenizer and Encoder/Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize : Transform Text --> Integers\n",
    "\n",
    "Short size of vocabulary but Long sequence length : Character-level Tokenizer\n",
    "Simple Tokeinizer : with a mapping of characters to integers using the position in chars (The one we will use)\n",
    "\n",
    "Long size of vocabulary but Short sequence length : sub-word/Word-level Tokenizer\n",
    "Google Tokenizer : SentencePiece : sub-words units level tokenizer\n",
    "OPENAI : Tiktoken : BPE Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# Characters of the text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder : Transform text --> Vectors/integers $\\newline$\n",
    "Decoder : Transform Vectors/integers --> text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48, 43, 1, 57, 59, 47, 57, 1, 27, 57, 47, 56, 47, 57]\n",
      "je suis Osiris\n"
     ]
    }
   ],
   "source": [
    "# Create the mapping from characters to integer\n",
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "def encode(s): return [stoi[c] for c in s]\n",
    "def decode(L): return \"\".join([itos[i] for i in L])\n",
    "\n",
    "print(encode(\"je suis Osiris\"))\n",
    "print(decode([48, 43, 1, 57, 59, 47, 57, 1, 27, 57, 47, 56, 47, 57]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using **PyTorch Tensors** to store text as **embeddings**\n",
    "\n",
    "- **Ease of use**: All modules (**nn.Embedding**, **nn.Linear**, **Transformer**, etc.) expect **torch.Tensor**.  \n",
    "\n",
    "- **GPU/TPU**: Once itâ€™s a **tensor**, it can be moved to **GPU/TPU** with `x.to(\"cuda\")` or `x.to(\"tpu\")`. **NumPy** only works on **CPU**.  \n",
    "\n",
    "- **Backward / Autograd**: **PyTorch tensors** keep track of operations for **automatic gradient computation**.  \n",
    "\n",
    "  **Example**:  \n",
    "  ```python\n",
    "  import torch\n",
    "\n",
    "  x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "  y = x * 2\n",
    "  y.backward(torch.ones_like(x))\n",
    "  print(x.grad)  # x remembers its expression with respect to y allowing it to compute x.grad automatically\n",
    "\n",
    "    ```\n",
    "- `x` remembers its expression with respect to `y`, which allows computing **x.grad** automatically.  \n",
    "\n",
    "- **Integrated tensors** in **PyTorch** (with **optimizers** like **Adam**, **SGD**, **loss functions**, training on **mini-batches**).  \n",
    "\n",
    "- **PyTorch** is optimized in **C++/CUDA** â†’ ultra-fast **matrix computations**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text))\n",
    "print(data.shape,data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting train and test\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Chunk = split the text** into smaller pieces (e.g. paragraphs of **300â€“500 tokens**).\n",
    "\n",
    "### ðŸ”¹ Why chunk?\n",
    "- **Context window limit**: LLMs can only read a fixed number of tokens (2k, 4k, 32k...).$\\newline$\n",
    "--> Chunking helps fit large texts into the model's context window.$\\newline$\n",
    "--> Attention mechanisms in transformers have quadratic complexity with respect to input length, so shorter inputs are more efficient.$\\newline$\n",
    "- **Memory efficiency**: smaller chunks use less memory during processing.\n",
    "- **Better local coherence**: smaller chunks keep the meaning clear.  \n",
    "- **Efficient search (RAG)**: embeddings are stored per chunk, making retrieval precise.  \n",
    "- **Performance**: faster similarity search + less dilution in attention.  \n",
    "\n",
    "Rule of thumb: chunks of **200â€“500 tokens** usually work best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: <bound method Kernel.raw_input of <ipykernel.ipkernel.IPythonKernel object at 0x7ff185012100>>, context: tensor([18])\n",
      "input: <bound method Kernel.raw_input of <ipykernel.ipkernel.IPythonKernel object at 0x7ff185012100>>, context: tensor([18, 47])\n",
      "input: <bound method Kernel.raw_input of <ipykernel.ipkernel.IPythonKernel object at 0x7ff185012100>>, context: tensor([18, 47, 56])\n",
      "input: <bound method Kernel.raw_input of <ipykernel.ipkernel.IPythonKernel object at 0x7ff185012100>>, context: tensor([18, 47, 56, 57])\n",
      "input: <bound method Kernel.raw_input of <ipykernel.ipkernel.IPythonKernel object at 0x7ff185012100>>, context: tensor([18, 47, 56, 57, 58])\n",
      "input: <bound method Kernel.raw_input of <ipykernel.ipkernel.IPythonKernel object at 0x7ff185012100>>, context: tensor([18, 47, 56, 57, 58,  1])\n",
      "input: <bound method Kernel.raw_input of <ipykernel.ipkernel.IPythonKernel object at 0x7ff185012100>>, context: tensor([18, 47, 56, 57, 58,  1, 15])\n",
      "input: <bound method Kernel.raw_input of <ipykernel.ipkernel.IPythonKernel object at 0x7ff185012100>>, context: tensor([18, 47, 56, 57, 58,  1, 15, 47])\n"
     ]
    }
   ],
   "source": [
    "# Exemple\n",
    "block_size = 8\n",
    "x = train_data[:block_size] # input of the transformer : first block size characters\n",
    "y = train_data[1:block_size+1] # next block size character offset by one --> target for each pos\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1] # all the tokens before the token at t\n",
    "    target = y[t]     # expected token at t\n",
    "    print(f'input: {context}, target: {y[t]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching\n",
    "\n",
    "- **Batch** = a set of sequences processed in parallel on the GPU.  \n",
    "- **Main purpose**: speed up training with GPU parallelization.  \n",
    "- Each sequence in the batch is **independent**:  \n",
    "  - The loss is calculated for each sequence, then combined to update the model weights.  \n",
    "- Batching is used **only to process multiple sequences in parallel** for GPU efficiency, **without sharing context between them**.\n",
    "- **Weight update**: after computing the losses for all the sequences in the batch, **the gradients are combined to update the modelâ€™s weights.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------Inputs-------------\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "-------------Targets-------------\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 4 # Number of sequences that will be processed in parallel\n",
    "block_size = 8 # Maximum content length for prediction\n",
    "\n",
    "def get_batch(split):\n",
    "    # Generate small batch of data of inputs x and target y\n",
    "    data = train_data if split =='train' else val_data\n",
    "    # Choice of random offset (starting point) duri,g training --> Avoid overfitting (<== if we start always at the same point)\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])       # context (stacked by row in a batch_size x block_size Tensor)\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])   # target/prediction\n",
    "    return x,y\n",
    "\n",
    "# Visualisation\n",
    "xb,yb = get_batch('train')\n",
    "print('-------------Inputs-------------')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('-------------Targets-------------')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "print('---------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = [24], target = 43\n",
      "input = [24, 43], target = 58\n",
      "input = [24, 43, 58], target = 5\n",
      "input = [24, 43, 58, 5], target = 57\n",
      "input = [24, 43, 58, 5, 57], target = 1\n",
      "input = [24, 43, 58, 5, 57, 1], target = 46\n",
      "input = [24, 43, 58, 5, 57, 1, 46], target = 43\n",
      "input = [24, 43, 58, 5, 57, 1, 46, 43], target = 39\n",
      "input = [44], target = 53\n",
      "input = [44, 53], target = 56\n",
      "input = [44, 53, 56], target = 1\n",
      "input = [44, 53, 56, 1], target = 58\n",
      "input = [44, 53, 56, 1, 58], target = 46\n",
      "input = [44, 53, 56, 1, 58, 46], target = 39\n",
      "input = [44, 53, 56, 1, 58, 46, 39], target = 58\n",
      "input = [44, 53, 56, 1, 58, 46, 39, 58], target = 1\n",
      "input = [52], target = 58\n",
      "input = [52, 58], target = 1\n",
      "input = [52, 58, 1], target = 58\n",
      "input = [52, 58, 1, 58], target = 46\n",
      "input = [52, 58, 1, 58, 46], target = 39\n",
      "input = [52, 58, 1, 58, 46, 39], target = 58\n",
      "input = [52, 58, 1, 58, 46, 39, 58], target = 1\n",
      "input = [52, 58, 1, 58, 46, 39, 58, 1], target = 46\n",
      "input = [25], target = 17\n",
      "input = [25, 17], target = 27\n",
      "input = [25, 17, 27], target = 10\n",
      "input = [25, 17, 27, 10], target = 0\n",
      "input = [25, 17, 27, 10, 0], target = 21\n",
      "input = [25, 17, 27, 10, 0, 21], target = 1\n",
      "input = [25, 17, 27, 10, 0, 21, 1], target = 54\n",
      "input = [25, 17, 27, 10, 0, 21, 1, 54], target = 39\n"
     ]
    }
   ],
   "source": [
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"input = {context.tolist()}, target = {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Bigram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram Model\n",
    "\n",
    "- A **bigram model** is a very simple language model that predicts a token based **only on the previous token**.  \n",
    "- **Unigram model** â†’ assumes each token is independent.  \n",
    "- **Bigram model** â†’ probability of a token depends only on the one before it.  \n",
    "- **N-gram model** â†’ generalization where the token depends on the previous *n-1* tokens.  \n",
    "\n",
    "\n",
    "### Formula\n",
    "\n",
    "For a sequence of tokens $ w_1, w_2, \\dots, w_n $:  \n",
    "\n",
    "$$\n",
    "P(w_1, w_2, \\dots, w_n) \\approx \\prod_{i=2}^n P(w_i \\mid w_{i-1})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Reminder: Role of `nn.Embedding` in PyTorch\n",
    "\n",
    "- **`nn.Embedding(num_embeddings, embedding_dim)`** = a **lookup table** (similar to dictionnary) that maps token indices to vectors.  \n",
    "\n",
    "- Each token (represented by an **integer index**) is associated with a **learnable vector**.  \n",
    "\n",
    "- In a **language model**:\n",
    "  - Input: token indices (`idx`).\n",
    "  - Output: embeddings (vectors) that represent each token.  \n",
    "\n",
    "- Example with `nn.Embedding(vocab_size, vocab_size)` in a Bigram model:\n",
    "  - Each token index is mapped to a **vector of size `vocab_size`**.\n",
    "  - This vector acts as the **logits** (scores) for predicting the **next token**.  \n",
    "  - Applying **softmax** on these logits gives a probability distribution over the vocabulary.  \n",
    "\n",
    "âž¡ï¸ In short: **`nn.Embedding` converts discrete tokens into continuous vectors** that the model can use for learning and prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLangaugeModels(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size,vocab_size)\n",
    "\n",
    "    def forward(self,idx, targets):\n",
    "\n",
    "        logits = self.token_embedding_table(idx) # size (B,T,C)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
